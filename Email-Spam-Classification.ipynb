{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email Spam Classifier \n",
    "\n",
    "* This dataset is a part of [CALO PROJECT](https://www.cs.cmu.edu/~./enron/).\n",
    "\n",
    "\n",
    "* It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 0.5M messages.\n",
    "\n",
    "\n",
    "* However for sole purpose of this project we are only using a chunk of the original dataset with labelled data.\n",
    "\n",
    "\n",
    "* The goal is to create a machine learning classifier that correctly classify whether an email is spam or ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: what up , , your cam babe what are yo...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: want to make more money ? order confi...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: food for thoughts [ join now - take a...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: your pharmacy ta would you want cheap...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: bigger breast just from a pill image ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               email class\n",
       "0  Subject: what up , , your cam babe what are yo...  spam\n",
       "1  Subject: want to make more money ? order confi...  spam\n",
       "2  Subject: food for thoughts [ join now - take a...  spam\n",
       "3  Subject: your pharmacy ta would you want cheap...  spam\n",
       "4  Subject: bigger breast just from a pill image ...  spam"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('email_train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have made 2 dataset first one is train_df for training and crossvalidtion and test_df as unseen data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: you need the capsule that will increa...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: correspondence assistant / manager he...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject:  c ; ia _ lis s _ o : ftab ' s place ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: new alternative remedy may help fight...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject:  the permanent solution to penis grow...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Class\n",
       "0  Subject: you need the capsule that will increa...  spam\n",
       "1  Subject: correspondence assistant / manager he...  spam\n",
       "2  Subject:  c ; ia _ lis s _ o : ftab ' s place ...  spam\n",
       "3  Subject: new alternative remedy may help fight...  spam\n",
       "4  Subject:  the permanent solution to penis grow...  spam"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('email_test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Train Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19998, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataset has almost 20k rows with 2 columns.\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam    11224\n",
       "ham      8774\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the distribution of emails in train dataset\n",
    "train_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset has 56.13% spam and 43.87% of ham.\n"
     ]
    }
   ],
   "source": [
    "spam_percent = (len(train_df[train_df['class'] == 'spam'])/len(train_df))*100\n",
    "ham_percent = (len(train_df[train_df['class']=='ham'])/len(train_df))*100\n",
    "print(\"The train dataset has {0:.2f}% spam and {1:.2f}% of ham.\".format(spam_percent,ham_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1141, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     767\n",
       "spam    374\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the distribution of emails in test dataset\n",
    "test_df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test dataset has 32.78% spam and 67.22% of ham.\n"
     ]
    }
   ],
   "source": [
    "spam_percent = (len(test_df[test_df['Class'] == 'spam'])/len(test_df))*100\n",
    "ham_percent = (len(test_df[test_df['Class']=='ham'])/len(test_df))*100\n",
    "print(\"The test dataset has {0:.2f}% spam and {1:.2f}% of ham.\".format(spam_percent,ham_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **The Emails Itself**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Subject: what up , , your cam babe what are you looking for ? if your looking for a companion for friendship , love , a date , or just good ole ' fashioned * * * * * * , then try our brand new site ; it was developed and created to help anyone find what they ' re looking for . a quick bio form and you ' re on the road to satisfaction in every sense of the word . . . . no matter what that may be ! try it out and youll be amazed . have a terrific time this evening copy and pa ste the add . ress you see on the line below into your browser to come to the site . http : / / www . meganbang . biz / bld / acc / no more plz http : / / www . naturalgolden . com / retract / counterattack aitken step preemptive shoehorn scaup . electrocardiograph movie honeycomb . monster war brandywine pietism byrne catatonia . encomia lookup intervenor skeleton turn catfish . \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.email[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's change the column names similar in both the datasets\n",
    "train_df.rename(columns={'class': 'label'}, inplace=True)\n",
    "test_df.rename(columns={'Class': 'label'}, inplace=True)\n",
    "test_df.rename(columns={'Text': 'email'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def removing_special_characters(sentence):\n",
    "    b = []\n",
    "    for i in sentence.split(' '):\n",
    "        if re.match('^[a-zA-Z]+',i):\n",
    "            b.append(i)\n",
    "    str1 = ' '.join(b)\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the special characters from the emails.\n",
    "train_df['email'] = train_df['email'].map(lambda x: removing_special_characters(x))\n",
    "test_df['email'] = test_df['email'].map(lambda x: removing_special_characters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the \"Subject:\" from the emails;\n",
    "train_df['email'] = train_df['email'].map(lambda x: x.lstrip('Subject:'))\n",
    "test_df['email'] = test_df['email'].map(lambda x: x.lstrip('Subject:'))\n",
    "#placing labels of 1 for spam and 0 for ham\n",
    "train_df['labels'] = train_df['label'].map(lambda x : 1 if x=='spam' else 0)\n",
    "test_df['labels'] = test_df['label'].map(lambda x : 1 if x=='spam' else 0)\n",
    "#droping the label columns\n",
    "train_df.drop('label', axis=1, inplace=True)\n",
    "test_df.drop('label', axis=1, inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' what up your cam babe what are you looking for if your looking for a companion for friendship love a date or just good ole fashioned then try our brand new site it was developed and created to help anyone find what they re looking for a quick bio form and you re on the road to satisfaction in every sense of the word no matter what that may be try it out and youll be amazed have a terrific time this evening copy and pa ste the add ress you see on the line below into your browser to come to the site http www meganbang biz bld acc no more plz http www naturalgolden com retract counterattack aitken step preemptive shoehorn scaup electrocardiograph movie honeycomb monster war brandywine pietism byrne catatonia encomia lookup intervenor skeleton turn catfish'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# email after preprocessing \n",
    "train_df.email[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We realized that all the labels of spams and hams are stacked together ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 1053,  1054,  1055,  1056,  1057,  1058,  1059,  1060,  1061,\n",
       "             1062,\n",
       "            ...\n",
       "            17385, 17386, 17387, 17388, 17389, 17390, 17391, 17392, 17393,\n",
       "            17394],\n",
       "           dtype='int64', length=8774)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['labels']==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,\n",
       "                9,\n",
       "            ...\n",
       "            19988, 19989, 19990, 19991, 19992, 19993, 19994, 19995, 19996,\n",
       "            19997],\n",
       "           dtype='int64', length=11224)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['labels']==1].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to shuffle the dataset for this we use  'sklearn.utils.shuffle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffling_dataset(df):\n",
    "    return shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = shuffling_dataset(train_df)\n",
    "train_df.reset_index(drop = True , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = shuffling_dataset(test_df)\n",
    "test_df.reset_index(drop = True , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>discounted prescription drugs online need vic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hershey s vs ghirardelli which chocolate do y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the delivery of the equipment you ordered is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>re uk portfolios and books setup in risktrac ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doorstep rac direction met today w causey and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>next generation ppv device next generation pp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fda approved we are one of the top sites on t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>new invoice for energy and weather vince plea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>manage your diabetes effortlessly with diabet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>updated q as for enron employees updated ques...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               email  labels\n",
       "0   discounted prescription drugs online need vic...       1\n",
       "1   hershey s vs ghirardelli which chocolate do y...       1\n",
       "2   the delivery of the equipment you ordered is ...       0\n",
       "3   re uk portfolios and books setup in risktrac ...       0\n",
       "4   doorstep rac direction met today w causey and...       0\n",
       "5   next generation ppv device next generation pp...       1\n",
       "6   fda approved we are one of the top sites on t...       1\n",
       "7   new invoice for energy and weather vince plea...       0\n",
       "8   manage your diabetes effortlessly with diabet...       1\n",
       "9   updated q as for enron employees updated ques...       0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the dataset using train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['email'], \n",
    "                                                    train_df['labels'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry:\n",
      "\n",
      "  re further follow up i have asked shelly jones to send me a complete list of the analysts from ou that will start in july i ll will contact her again today if she doesn t get back with me i think it would be a great idea to invite ted his e mail is tjacobs ou edu it might also be a good idea to invite george hope i will also ask shelly if they have assigned a recruiter to ou yet this would be a great opportunity for the individual to meet with the potential recruiting team p s if i am thinking too much and the list is growing larger than what you had anticipated let me know\n",
      "\n",
      "\n",
      "X_train shape:  (14998,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train first entry:\\n\\n', X_train.iloc[0])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before training our model we need to preprocess certain features from our dataset. One of them being the ''uppercase-lowercase'' issue, frequency count of words in a spam or ham, and the distribution of important words ignoring words like \"the\" , \"a\" ,\"an\" etc.\n",
    "\n",
    "\n",
    "\n",
    "* For that we use an approach known as **bag of word** approach. The bag of words approach is commonly used way to represent text for use in machine learning which ignores structures and only counts how often each word occurs . It allows us to use bags of words approach by converting a collections of text document into matrix of token count.\n",
    "\n",
    "\n",
    "\n",
    "* Sklearn has a method known as **CountVectorisor**. Fitting the count vectorisor consist of the tokenisation of the trained data and building of the vocabulary. It fits by finding all sequnces of characters of atleast two letters or numbers separated by word boundaries.Convert everything to lowercase and builds a vocabulary using these tokens. \n",
    "\n",
    "\n",
    "* The result of CountVectorisor is a **high dimensional sparse matrix** containing count of each word in the dataset stored in a  compressed row format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'afflicted',\n",
       " 'ammeter',\n",
       " 'archdioceseshmuel',\n",
       " 'availabilities',\n",
       " 'becloudherdsman',\n",
       " 'ble',\n",
       " 'brittany',\n",
       " 'cannibals',\n",
       " 'chaneys',\n",
       " 'clenches',\n",
       " 'computadores',\n",
       " 'corrodedelirious',\n",
       " 'cyberage',\n",
       " 'demigodimmobility',\n",
       " 'disarrange',\n",
       " 'dripping',\n",
       " 'ekj',\n",
       " 'equipment',\n",
       " 'exxon',\n",
       " 'fixefad',\n",
       " 'fsj',\n",
       " 'giris',\n",
       " 'gujarantee',\n",
       " 'heritage',\n",
       " 'humour',\n",
       " 'incompliance',\n",
       " 'inversion',\n",
       " 'josie',\n",
       " 'kristina',\n",
       " 'lhe',\n",
       " 'lrqvqk',\n",
       " 'masque',\n",
       " 'miceproletariat',\n",
       " 'motley',\n",
       " 'nero',\n",
       " 'oak',\n",
       " 'organisation',\n",
       " 'passages',\n",
       " 'phonal',\n",
       " 'portugese',\n",
       " 'promo',\n",
       " 'quothang',\n",
       " 'regular',\n",
       " 'riddel',\n",
       " 'sailor',\n",
       " 'selma',\n",
       " 'silt',\n",
       " 'sowohl',\n",
       " 'strategic',\n",
       " 'szablya',\n",
       " 'thi',\n",
       " 'transfuse',\n",
       " 'und',\n",
       " 'vdaerpa',\n",
       " 'wachiya',\n",
       " 'winter',\n",
       " 'yazilimlari']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57754"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14998x57754 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 739659 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9834175772996927\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Classification Report------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98      2173\n",
      "          1       0.98      0.99      0.99      2827\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"\\n----------Classification Report------------------------------------\")\n",
    "print(classification_report(y_test,predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(vect.transform([\"Sir you have a meeting tomorrow \",\n",
    "                                    'limited seats available,Apply quickly to avail the offer'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score for unseen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9530307678363512\n"
     ]
    }
   ],
   "source": [
    "unseen_predictions = model.predict(vect.transform(test_df['email']))\n",
    "print('AUC: ', roc_auc_score(test_df['labels'], unseen_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95       767\n",
      "          1       0.84      1.00      0.91       374\n",
      "\n",
      "avg / total       0.95      0.94      0.94      1141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df['labels'], unseen_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(vect.transform([\"Send me those files you told yesterday\",\n",
    "                                    'Sir you have won 1milion dollor'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features that led to high prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['enron' 'vince' 'louise' 'attached' 'thanks' 'congratulations' 'doc'\n",
      " 'meeting' 'hotmail' 'dave']\n",
      "\n",
      "Largest Coefs: \n",
      "['software' 'remove' 'online' 'mobile' 'hello' 'viagra' 'http' 'click'\n",
      " 'found' 'sex']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting some more context in our model's predictions\n",
    "\n",
    "#### Using TF-IDF\n",
    "\n",
    "* The term frequency inverse document frequency allows us to weight terms based on how important they are to the document.\n",
    "\n",
    "\n",
    "* High weight is given to terms that appear often in a particular document but doesn't appear often in the corpus. Features with low TF-IDF are either commonly used across all document or rarely used and only in long document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10677"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9809740925996134\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Classification Report------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98      2173\n",
      "          1       0.97      1.00      0.99      2827\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----------Classification Report------------------------------------\")\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(vect.transform([\"Send me those files you told yesterday\",\n",
    "                                    'Sir you have won 1milion dollor'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9452080820475636\n"
     ]
    }
   ],
   "source": [
    "unseen_predictions = model.predict(vect.transform(test_df['email']))\n",
    "print('AUC: ', roc_auc_score(test_df['labels'], unseen_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94       767\n",
      "          1       0.82      1.00      0.90       374\n",
      "\n",
      "avg / total       0.94      0.93      0.93      1141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df['labels'], unseen_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['painter' 'framemaker' 'xara' 'golive' 'buiider' 'cooi' 'paqemaker'\n",
      " 'publishinq' 'ulead' 'uiead']\n",
      "\n",
      "Largest tfidf: \n",
      "['love' 'fax' 'dz' 'anderson' 'fyi' 'ecclesiastic' 'note' 'want' 're'\n",
      " 'spam']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['enron' 'vince' 'thanks' 'attached' 'louise' 'the' 'will' 'gas' 'pm'\n",
      " 'please']\n",
      "\n",
      "Largest Coefs: \n",
      "['your' 'http' 'here' 'software' 'you' 'online' 'click' 'no' 'more' 'our']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectoriser with N-grams\n",
    "\n",
    "* It adds sequences of word features known as bigrams , which count pair of adjacent words could be given as features like working versus not working . A trigrams which gives us triplets of adjacent words ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35841"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9846916957332904\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Classification Report------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98      2173\n",
      "          1       0.98      0.99      0.99      2827\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----------Classification Report------------------------------------\")\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['enron' 'vince' 'louise' 'attached' 'thanks' 'doc' 'congratulations'\n",
      " 'pictures' 'fyi' 'promotion']\n",
      "\n",
      "Largest Coefs: \n",
      "['software' 'remove' 'online' 'your' 'http' 'hello' 'goodbye'\n",
      " 'mobile email' 'phone mobile' 'viagra']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.955638329765947\n"
     ]
    }
   ],
   "source": [
    "unseen_predictions = model.predict(vect.transform(test_df['email']))\n",
    "print('AUC: ', roc_auc_score(test_df['labels'], unseen_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95       767\n",
      "          1       0.85      1.00      0.92       374\n",
      "\n",
      "avg / total       0.95      0.94      0.94      1141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df['labels'], unseen_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(vect.transform(['Hi All,As a part of Campus 2019, we are making our muAPT registration process simpler and faster!Regarding the same, we need all the students who have an interest in applying for Mu Sigma to update their LinkedIn profile as we will be routing the registrations through the same.Please ensure all the students update their e-mail id on LinkedIn as all subsequent communication regarding the Selection process will be done through that.Regards,',\n",
    "                                    'Dear Saheel Ahmed,Congratulations! You have qualified for Round 2 of the HackWithInfy contest. With this achievement you are now a step closer to the Grand Finale where you can win cash prizes worth INR 350,000 and a pre-placement interview at Infosys.It gives us immense pleasure to share that you are one of the few who have made this mark to be among the top 5000 programmers to qualify for Round 2 of #HackWithInfy. Now the competition will only get tougher and it is time for you to prepare well.Top 100 finalists who clear Round 2 will get an opportunity to compete in the Grand Finale, at Infosys Pune and win a pre-placement interview (PPI) for the Power Programmer role. The next top 500 finalists will get a PPI for the Systems Engineer role.'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(vect.transform([\"Congratulations you are appointed as the new intern\",\n",
    "                                    'Sir you have won 1milion dollor'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
